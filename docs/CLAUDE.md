# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

AI-Economic-Interpreter is an integrated economic analysis platform that collects major market indices (KOSPI, NASDAQ) along with news/policy/fund flows, and provides insights through sequential discussion by three virtual experts (Eco, Firm, House). The system combines RAG (Retrieval-Augmented Generation) with role-specific LoRA adapters to generate evidence-based economic interpretations.

## Architecture

The system is composed of four main services:

1. **Frontend** (Next.js 14, port 3000): Dashboard UI with real-time streaming responses
2. **Backend** (Express + TypeScript, port 3001): API orchestration, RAG logic, and expert flow coordination
3. **AI Core** (FastAPI, ports 8001-8003): Three role-specific inference servers (Eco/Firm/House) with LoRA adapters
4. **Market API** (FastAPI, port 8000): Yahoo Finance proxy with caching

### Expert Chain Flow

The system uses a sequential chain where each expert builds on the previous analysis:
- **Eco**: Macroeconomic analysis (GDP, interest rates, exchange rates, policies)
- **Firm**: Corporate/sector analysis (financial metrics, industry trends, specific companies)
- **House**: Household finance strategy (portfolios, risk management, investment allocation)

Request flow: User query → Backend planner selects roles → Each role performs `buildRoleQuery → searchRAG → genDraft` → Editor synthesizes final response

## Development Commands

### One-Command Startup

```bash
./run.sh
```

This launches all services in order (market_api → ai-core → backend → frontend) with logs to `logs/*.log`. Press Ctrl+C for safe shutdown.

### Individual Service Commands

```bash
# Backend (Express API)
cd backend
npm install
npm run dev      # Development with hot reload
npm run build    # TypeScript compilation
npm start        # Production mode

# Frontend (Next.js)
cd frontend
npm install
npm run dev      # Development server
npm run build    # Production build
npm start        # Serve production build
npm run lint     # ESLint

# AI Core (multi-role inference)
cd ai
python main.py   # Launches eco(8001), firm(8002), house(8003)

# Market API
cd market_api
python -m uvicorn app:app --host 127.0.0.1 --port 8000 --reload

# RAG Index Setup
cd RAG_zzin
./setup_and_ingest.sh   # Creates venv, installs deps, builds vector index
```

### Testing

```bash
# Backend health check
curl http://localhost:3001/health

# Sequential expert flow test
curl -s http://localhost:3001/ask \
  -H "Content-Type: application/json" \
  -d '{"q":"미국 금리 인상 후 국내 가계 전략은?","roles":["eco","firm","house"],"mode":"sequential"}' \
  | jq '.cards[].title'

# Daily insight (news + indices)
curl -s http://localhost:3001/insight/daily?limit=3 | jq '.summary'
```

## Key Implementation Details

### Backend Entry Point

`backend/src/index.ts` sets up Express with these routes:
- `/ask`: Main expert flow endpoint (backend/src/routes/ask.ts)
- `/ask/stream`: NDJSON streaming variant
- `/timeseries`: Market data proxy (backend/src/routes/timeseries.ts)
- `/insight/daily`: Daily summary generation (backend/src/routes/insight.ts)
- `/health`: Health check

### Role Selection Logic

`backend/src/routes/ask.ts` contains `selectRoles()` which:
- Automatically detects required experts based on query keywords
- Supports explicit role specification via `roles` parameter
- Enforces allowed role combinations (7 valid paths)
- Defaults to `['eco']` for ambiguous queries

### AI Core Architecture

`ai/main.py` spawns three FastAPI processes, each loading:
- Base model: Qwen3-0.6B (configurable via `MODEL_ID`)
- Role-specific LoRA adapter from `ai/{role}/lora/qwen3_0p6b_lora_{role}/final`
- Backend type: RBLN (default) or PyTorch (via `MODEL_BACKEND=torch`)

The backend calls these services via `backend/src/ai/provider_local.ts` which manages:
- Request routing to correct port based on role
- LoRA adapter resolution via `ROLE_LORA_NAMES`
- Token limits (`ASK_ROLE_MAX_TOKENS`, `ASK_EDITOR_MAX_TOKENS`)

### RAG Pipeline

RAG data is managed in `RAG_zzin/`:
- Source data: `RAG_zzin/data/` (JSON/JSONL files)
- Vector index: Generated by `RAG_zzin/ingest.py`
- Retrieval: `backend/src/ai/rag.ts` calls Python retrieval service
- Query building: `buildRoleQuery()` in ask.ts combines role keywords with previous card summaries

To add RAG data:
1. Add JSON/JSONL files to `RAG_zzin/data/`
2. Run `./RAG_zzin/setup_and_ingest.sh`
3. Restart backend to use updated index

### Sequential Mode

When `mode=sequential`:
1. Eco analyzes the question first with RAG evidence
2. Firm receives Eco's output as context, performs its own RAG search
3. House receives both Eco and Firm outputs, performs final analysis
4. Editor synthesizes all three perspectives into unified response

Implementation: `backend/src/routes/ask.ts` orchestrates the sequential calls with `genDraft()` for each role and `genEditor()` for final synthesis.

## Environment Variables

Core variables (see `.env.example` in each directory):

```bash
# Ports
MARKET_API_PORT=8000
AI_PORT=8008          # Not used - individual roles use 8001-8003
BACKEND_PORT=3001
FRONTEND_PORT=3000    # Set via PORT in frontend

# Backend → AI routing
ROUTER_AI_BASE=http://localhost:8008  # Base URL (individual ports derived)

# Model configuration
MODEL_BACKEND=rbln    # or "torch"
MODEL_ID=Qwen/Qwen3-0.6B
ECO_MODEL_ID=/path/to/compiled_eco    # Override per role
FIRM_MODEL_ID=/path/to/compiled_firm
HOUSE_MODEL_ID=/path/to/compiled_house

# Token limits
ASK_ROLE_MAX_TOKENS=4096
ASK_EDITOR_MAX_TOKENS=4096

# Frontend
NEXT_PUBLIC_API_BASE=http://localhost:3001
```

## Project Structure

```
ai/                     AI Core (FastAPI, multi-role LoRA servers)
  ├── main.py              Entry point spawning eco/firm/house
  ├── server_base.py       Shared server logic
  ├── eco/lora/            Eco LoRA adapter
  ├── firm/lora/           Firm LoRA adapter
  └── house/lora/          House LoRA adapter

backend/                Backend API (Express + TypeScript)
  └── src/
      ├── index.ts         Express app setup
      ├── types.ts         Shared TypeScript types
      ├── routes/
      │   ├── ask.ts       Expert flow orchestration
      │   ├── insight.ts   Daily summary generation
      │   ├── timeseries.ts Market data proxy
      │   └── health.ts    Health endpoint
      ├── ai/
      │   ├── bridge.ts    AI service interface
      │   ├── provider_local.ts  HTTP calls to AI Core
      │   ├── rag.ts       RAG retrieval interface
      │   └── prompts.ts   Prompt templates
      └── services/
          └── news.ts      News fetching logic

frontend/               Next.js UI
  └── src/
      ├── app/           App router pages
      ├── components/    React components
      ├── hooks/         Custom hooks
      └── lib/           Utilities

market_api/            Market data service
  └── app.py            Yahoo Finance proxy

RAG_zzin/              RAG data and indexing
  ├── data/             Source documents (JSON/JSONL)
  ├── ingest.py         Index builder
  ├── retriever.py      Retrieval logic
  └── setup_and_ingest.sh  Automated setup script

logs/                  Service logs (created by run.sh)
```

## Type Definitions

Key types from `backend/src/types.ts`:

```typescript
type Role = 'eco' | 'firm' | 'house' | 'combined';
type Mode = 'auto' | 'parallel' | 'sequential';

interface AskInput {
  q: string;           // User question
  mode?: Mode;         // Defaults to 'auto'
  prefer?: Role[];     // Hint for role selection
  roles?: Role[];      // Explicit role override
}

interface Card {
  type: Role;
  title: string;
  content: string;
  points?: string[];
  sources?: Source[];
  badges?: string[];
  conf?: number;
}

interface AskOutput {
  cards: Card[];
  metrics?: { ttft_ms?: number; tps?: number; tokens?: number };
  meta?: {
    mode: Mode;
    roles: Role[];
    provider?: string;
    plan_reason?: string;
  };
}
```

## Common Patterns

### Adding a New Expert Role

1. Train LoRA adapter, save to `ai/{newrole}/lora/qwen3_0p6b_lora_{newrole}/final`
2. Update `ROLE_PORTS` in `ai/main.py` with new port
3. Add role to `AskRole` type in `backend/src/ai/bridge.ts`
4. Update `ROLE_LORA_NAMES` mapping
5. Extend role selection logic in `backend/src/routes/ask.ts`
6. Add to `ALLOWED_PATHS` array

### Modifying Role Selection Heuristics

Edit `selectRoles()` in `backend/src/routes/ask.ts`:
- Add keyword patterns for role detection
- Adjust intent routing rules (e.g., specific Korean phrasings)
- Modify role combination logic
- Ensure result passes through `enforceAllowed()` validation

### Customizing Prompts

Prompt templates are in `backend/src/ai/prompts.ts`:
- `draftPrompt`: Individual expert generation
- `editorPrompt`: Final synthesis
- `routerPrompt`: Role selection (if AI-based routing enabled)
- `dailyInsightPrompt`: Daily summary generation
- `marketSummaryPrompt`: Market index summarization

### Compiling RBLN Models

For deployment with RBLN accelerator:

```bash
cd ai
python compile_rbln_model.py Qwen/Qwen3-0.6B \
  --max-seq-len 8192 \
  --lora eco=ai/eco/lora/qwen3_0p6b_lora_eco/final \
  --lora firm=ai/firm/lora/qwen3_0p6b_lora_firm/final \
  --lora house=ai/house/lora/qwen3_0p6b_lora_house/final
```

Update environment variables to point to compiled paths.

## Debugging

### Checking Service Status

```bash
# View logs
tail -f logs/backend.log
tail -f logs/ai-core.log
tail -f logs/market-api.log
tail -f logs/frontend.log

# Test individual AI role servers
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"GDP가 뭐야?"}]}'

# Test RAG retrieval
cd RAG_zzin
python -m RAG_zzin.cli_query "금리 인상"
```

### Common Issues

- **"No RAG results"**: Check that `RAG_zzin/setup_and_ingest.sh` has been run and index files exist
- **Sequential mode returns empty cards**: Verify RAG data includes relevant keywords for all three roles
- **AI Core timeout**: Check `logs/ai-core.log` for model loading errors; RBLN models require RBLN runtime
- **Port conflicts**: Ensure no other services use ports 3000, 3001, 8000, 8001-8003

## Notes

- Frontend uses local storage for conversation history; no server-side session management
- The system is designed for localhost development; production deployment requires session handling and external access configuration
- RBLN backend provides ~10x faster inference than PyTorch CPU; requires RBLN SDK installed
- RAG data should be refreshed periodically by re-running ingestion script
